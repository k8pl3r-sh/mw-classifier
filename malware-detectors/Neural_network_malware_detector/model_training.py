import numpy as np
import mmh3
import re
import os
import tensorflow as tf
from tensorflow.keras.models import load_model
from model_architecture import my_model

def read_file(sha, dir):
    file_path = os.path.join(dir, sha)
    with open(file_path, 'r', encoding='utf-8', errors='replace') as fp:
        file = fp.read()
    return file

def extract_features(sha, path_to_files_dir, hash_dim=1024, split_regex=r"\s+"):
    file = read_file(sha=sha, dir=path_to_files_dir)
    tokens = re.split(pattern=split_regex, string=file)
    token_hash_buckets = [(mmh3.hash(w) % (hash_dim - 1) + 1) for w in tokens]
    token_bucket_counts = np.zeros(hash_dim)
    buckets, counts = np.unique(token_hash_buckets, return_counts=True)
    for bucket, count in zip(buckets, counts):
        token_bucket_counts[bucket] = count
    return np.array(token_bucket_counts)

def my_generator(benign_files, malicious_files, path_to_benign_files, path_to_malicious_files, batch_size, features_length=1024):
    n_samples_per_class = batch_size // 2
    assert len(benign_files) >= n_samples_per_class
    assert len(malicious_files) >= n_samples_per_class
    while True:
        ben_features = [
            extract_features(sha, path_to_files_dir=path_to_benign_files, hash_dim=features_length)
            for sha in np.random.choice(benign_files, n_samples_per_class, replace=False)
        ]
        mal_features = [
            extract_features(sha, path_to_files_dir=path_to_malicious_files, hash_dim=features_length)
            for sha in np.random.choice(malicious_files, n_samples_per_class, replace=False)
        ]
        all_features = ben_features + mal_features
        labels = [0] * n_samples_per_class + [1] * n_samples_per_class

        idx = np.random.permutation(batch_size)
        all_features = np.array([np.array(all_features[i]) for i in idx])
        labels = np.array([labels[i] for i in idx])
        yield all_features, labels

def make_training_data_generator(features_length, batch_size):
    path_to_training_benign_files = 'data/html/benign_files/training/'
    path_to_training_malicious_files = 'data/html/malicious_files/training/'

    train_benign_files = os.listdir(path_to_training_benign_files)
    train_malicious_files = os.listdir(path_to_training_malicious_files)

    training_generator = my_generator(
        benign_files=train_benign_files,
        malicious_files=train_malicious_files,
        path_to_benign_files=path_to_training_benign_files,
        path_to_malicious_files=path_to_training_malicious_files,
        batch_size=batch_size,
        features_length=features_length
    )
    return training_generator

def get_validation_data(features_length, n_validation_files):
    path_to_validation_benign_files = 'data/html/benign_files/validation/'
    path_to_validation_malicious_files = 'data/html/malicious_files/validation/'

    val_benign_files = os.listdir(path_to_validation_benign_files)
    val_malicious_files = os.listdir(path_to_validation_malicious_files)

    validation_data = next(my_generator(
        benign_files=val_benign_files,
        malicious_files=val_malicious_files,
        path_to_benign_files=path_to_validation_benign_files,
        path_to_malicious_files=path_to_validation_malicious_files,
        batch_size=n_validation_files,
        features_length=features_length
    ))
    return validation_data

def example_code_with_validation_data(model, training_generator, steps_per_epoch, features_length, n_validation_files):
    validation_data = get_validation_data(features_length, n_validation_files)
    model.fit(
        training_generator,
        steps_per_epoch=steps_per_epoch,
        epochs=10,
        validation_data=validation_data,
        verbose=1
    )
    return model

if __name__ == '__main__':
    features_length = 1024
    num_obs_per_epoch = 5000
    batch_size = 128

    model = my_model(input_length=features_length)

    training_generator = make_training_data_generator(batch_size=batch_size, features_length=features_length)

    model.fit(
        training_generator,
        steps_per_epoch=num_obs_per_epoch // batch_size,
        epochs=10,
        verbose=1
    )

    validation_data = get_validation_data(features_length=features_length, n_validation_files=1000)

    model.fit(
        training_generator,
        steps_per_epoch=num_obs_per_epoch // batch_size,
        epochs=10,
        validation_data=validation_data,
        verbose=1
    )

    # Save the model in the new Keras format
    model.save('my_model.keras')

    # Load the model back into memory from the file
    same_model = load_model('my_model.keras')
