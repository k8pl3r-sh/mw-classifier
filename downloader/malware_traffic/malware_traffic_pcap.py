#!/usr/bin/python3
import os
import requests
from bs4 import BeautifulSoup
import zipfile
import time

FROM = '2013'
TO = '2024'

URL = "https://www.malware-traffic-analysis.net/"
FILE_LOG = "downloaded_zip.txt"


def check_previous_download(filename: str, file_log: str) -> bool:
    """
    Check if the file has already been downloaded, return True if it has, False otherwise
    Parameters
    ----------
    filename : name of the potential file to downloader
    file_log : logs of previous downloaded files
    """
    try:
        with open(file_log, "r") as f:
            lines = f.readlines()
            for line in lines:

                if filename in line.strip():
                    print(f"Already downloaded {filename}")
                    return True

    except FileNotFoundError:
        return False


def extract_links(response: requests.Response) -> list | None:
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, "html.parser")

        links = soup.find_all('a', href=True)
        return links
    else:
        return None


def get_yearly_samples_url(url: str, year: str) -> list[str]:
    """
    Return all URLs of samples, ending by a ZIP extension from the given URL
    Parameters
    ----------
    url : index URL
    year : date from which we want samples
    -------

    """
    full_url = url + year + '/'
    response = requests.get(full_url)
    path = []

    links = extract_links(response)
    if links is not None:
        for link in links:
            href = link['href']
            if 'index.html' in href and '..' not in href:
                path.append(href.split('i')[0])

    full_url_dl = []

    for element in path:
        response = requests.get(full_url + element)

        links = extract_links(response)

        if links is not None:
            for link in links:
                href = link['href']
                if '.zip' in href:
                    # print(f"{full_url + element + href}")
                    full_url_dl.append(full_url + element + href)
    print(f"FULL URL : {full_url_dl}")
    return full_url_dl


def extract_password(filename: str) -> bytes:
    """
    Construct a password from the filename according to malware-traffic nomenclature
    """
    fs = filename.split('-')
    # fs : year, month, day...
    password = 'infected_' + fs[0] + fs[1] + fs[2]
    print(f"password {password} with url {filename}")
    return password.encode()


def download_samples(link: str, log_file: str = FILE_LOG):
    """

    Parameters
    ----------
    link : link where the sample must be downloaded
    log_file : file to check if a sample has been already downloaded

    -------

    """
    time.sleep(1)  # Delay to avoid being banned
    r = requests.get(link)
    filename = link.split('/')[-1]
    if not check_previous_download(filename, log_file):

        with open(link.split('/')[-1], 'wb') as f:
            f.write(r.content)

        print(f"File downloaded: {filename}")

        with open(log_file, "a") as f:
            f.write(f"{filename}\n")

        # Extract samples
        directory = "samples"
        os.makedirs(directory, exist_ok=True)

        with zipfile.ZipFile(filename, 'r') as zip_ref:
            zip_ref.extractall(directory, pwd=extract_password(filename))

        print("Zip file extracted successfully.")
        os.remove(filename)  # Remove the ZIP archive


def extract_zip_files(directory="."):
    # Walk through the directory
    for root, dirs, files in os.walk(directory):
        for file in files:
            if file.endswith(".zip"):
                zip_path = os.path.join(root, file)
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    print(f"PATH : {zip_path}")
                    zip_ref.extractall(zip_path, pwd=extract_password(zip_path))
                print(f"Extracted {file} in {zip_path}")
                os.remove(zip_path)


if __name__ == '__main__':
    for i in range(int(TO) - int(FROM)):
        full_urls = get_yearly_samples_url(URL, str(int(FROM) + i))
        # print(full_urls)
        for element in full_urls:
            download_samples(element, FILE_LOG)
        print(f"Downloaded all samples from {str(int(FROM) + i)}")
